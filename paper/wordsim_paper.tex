\def\year{2015}
\def\thetitle{Improving Word Vectors by Adding Lexical Knowledge}

% Update this when our score changes. Maybe we can even automate this.
\def\scoreRW{.584}
\def\scoreMEN{.854}

\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{CJKutf8}
\usepackage{url}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title \thetitle
/Author Rob Speer, Joshua Chin, and Catherine Havasi}
\setcounter{secnumdepth}{0}
\bibliographystyle{aaai}

\title{\thetitle}
%\author{Robert Speer\\
%    Luminoso Technologies, Inc.\\
%    675 Massachusetts Ave.\\
%    Cambridge, MA 02139\\
%    \texttt{rspeer@luminoso.com}
%\And
%    Joshua Chin\\
%    {\em academic address goes here}
%\And
%    Catherine Havasi\\
%    Luminoso Technologies, Inc.\\
%    675 Massachusetts Ave.\\
%    Cambridge, MA 02139\\
%    \texttt{havasi@luminoso.com}
%}

\begin{document}
\maketitle
\begin{abstract}
Embeddings of natural-language words in a vector space are useful for a variety
of tasks in semantics. Embeddings learned from unstructured text can be
combined with structured knowledge to achieve performance that neither source
would achieve on its own, as shown by \citeauthor{faruqui2014retrofitting}
\shortcite{faruqui2014retrofitting}. In this paper, we show the effectiveness
of combining the GloVe embeddings \cite{pennington2014glove} with the
crowd-sourced semantic network ConceptNet \cite{speer2012conceptnet}, with
some changes to the GloVe embeddings and \citeauthor{faruqui2014retrofitting}'s
``retrofitting'' algorithm. The resulting vector space has a larger vocabulary
than either source, and achieves state-of-the-art performance on multiple word
similarity evaluations, including an unprecedented score of $\rho = \scoreRW{}$
on \citeauthor{luong2013rw}'s rare words evaluation.
\end{abstract}

\section{Introduction}
blah blah blah

\section{Background}

\subsection{GloVe}
\ldots introduce GloVe \ldots % TODO

Learning processes such as GloVe produce results that can conveniently be
re-used in other projects, because they output a labeled matrix of the learned
embeddings of each word in the vocabulary.
\citeauthor{faruqui2014retrofitting} \shortcite{faruqui2014retrofitting}
introduced the ``retrofitting'' procedure, which adjusts such dense matrices of
embeddings based on external knowledge from a sparse semantic network.

\citeauthor{faruqui2014retrofitting} used PPDB \cite{ganitkevitch2013ppdb} as
the external source of semantic knowledge, but we found that ConceptNet
\cite{speer2012conceptnet}, a resource which combines a variety of forms of
crowd-sourced and expert-created knowledge, should fit well with this process
while greatly increasing the scope of its vocabulary.

\subsection{ConceptNet}
ConceptNet 5 \cite{speer2012conceptnet} is a semantic network of terms
connected by labeled relations. Its terms are words or multiple-word phrases
in a variety of natural language. For continuity with previous work,
these terms are often referred to as {\em concepts}.

ConceptNet originated as a machine-parsed version of the early crowd-sourcing
project called Open Mind Common Sense (OMCS) \cite{singh2002omcs}, and has expanded
to include several other data sources, both crowd-sourced and expert-created.
The data sources included in ConceptNet 5.4 are:

\begin{itemize}
\item Knowledge collected as English sentences on the original OMCS website,
    and later parsed with a pattern-matching parser
\item Sister projects to OMCS that collected similar sentences in Portuguese,
    Dutch, and some Korean and Japanese \cite{anacleto2006portuguese}
    \cite{eckhardt2008dutch} \cite{TODO-globalmind}
\item ``Games with a purpose'' that collect relational knowledge, such as
    Verbosity \cite{vonahn2006verbosity} in English, {\em nadya.jp}
    \cite{nakahara2011nadya} in Japanese, and the PTT pet game \cite{kuo2009petgame}
    in Taiwanese Chinese
\item WordNet 3.0 \cite{fellbaum1998wordnet}
\item A parsed version of Wiktionary, whose parser is developed as part of the
    ConceptNet 5 codebase
\item JMDict \cite{TODOjmdict}, a Japanese multilingual dictionary
\item OpenCyc \cite{TODOcyc} as represented in Umbel \cite{TODOumbel}
\end{itemize}

ConceptNet 5.4 also includes a mapping of DBPedia \cite{auer2007dbpedia} into
its term space, but it is not used for word associations, so its presence does
not affect the results in this paper.

% TODO: ConceptNet image


\section{Methods}

Here, we create a matrix that assigns embeddings in a 300-dimensional vector
space to terms by combining data from ConceptNet and GloVe. The resulting word
embeddings incorporate the strengths of both ConceptNet and GloVe, and extend
their vocabulary to the union of ConceptNet and GloVe, allowing embeddings that
were originally only trained on English to be extended to other languages.


\subsection{Standardizing text}

As \citeauthor{levy2015embeddings} \shortcite{levy2015embeddings} note,
``[\ldots] much of the performance gains of word embeddings are due to certain
system design choices and hyperparameter optimizations, rather than the
embedding algorithms themselves.'' While it is presented as a negative result,
this simply emphasizes the importance of these system design choices.

Indeed, we have found that choices about how to handle text have a significant
impact on evaluation results. Additionally, we can only properly combine two
resources with a method such as retrofitting if their string representations
are comparable to each other.

The operations we apply to text would usually be called ``normalization'',
but that word also refers to what we'll be doing to our vectors, so both
here and in the code we call it ``standardization'' instead.

\subsubsection{How ConceptNet is standardized}

Here are the standardizations applied to all terms in ConceptNet 5.4, as
implemented in the {\tt conceptnet5.nodes.normalized\_concept\_uri} function
of ConceptNet 5.4:

\begin{itemize}
\item The words and phrases that the data sources provide are first run
    through the {\tt ftfy.fix\_text} function, in version 4.0 of the Python
    module {\tt ftfy}.\footnote{
        \url{http://github.com/LuminosoInsight/python-ftfy}
    } This corrects ``mojibake'' (a Unicode glitch that arises when reading
    text in the wrong encoding), as well as applying NFC normalization.\footnote{
        For example, we consider ``ni\~{n}o'' to be a single
        label, regardless of whether it is spelled as the four codepoints
        {\tt n i \~{n} o}, the five codepoints {\tt n i n \textasciitilde{} o}
        where the fourth is a combining character,
        or the five corrupted codepoints {\tt n i \~{A} \textpm{} o}.
    }
\item The text is tokenized using a regular expression that splits it at
    characters whose major Unicode category is Z (whitespace), P (punctuation),
    S (symbols), or C (control characters).
\item If the text is English, each token is lemmatized using a modification of
    WordNet's Morphy algorithm, which appears in the {\tt conceptnet5.language.english}
    module of ConceptNet 5.4's Python code. This converts inflected words such
    as ``creating'' to their root words that appear in WordNet, such as
    ``create''.
    % Because most data sources are not tagged with parts of speech,
    % it uses heuristics to choose which part of speech to lemmatize the word as,
    % plus exceptions that allow it to choose more common roots when Morphy would
    % give multiple options.
\item If the text is English, a very small list of stopwords (``a'', ``an'',
    and ``the'') are removed, as well as ``to'' if it is the first word.
\item Spaces and slashes are replaced by underscores.
\item The text is lowercased according to the Unicode case-mapping algorithm
    that is implemented in Python 3.4.
\item The resulting standardized text is tagged with its language and turned
    into a URI: the phrase ``giving an example'', for example, becomes
    {\tt /c/en/give\_example}.
\end{itemize}

The resulting URIs are not entirely meant for human consumption, and are
sometimes not grammatically accurate -- the term ``ground zero'' becomes
{\tt /c/en/grind\_zero}, for example -- but they unify surface texts that may
appear in slightly different forms in different resources.

\subsubsection{How GloVe is standardized}

The 42-billion-token version of GloVe, whose results are published in
\cite{pennington2014glove}, has applied some simple standardizations
to the text it extracts from Common Crawl:

\begin{itemize}
\item The text is tokenized with the Stanford parser. Only single tokens are
    used in GloVe.
\item The words are lowercased using a Unicode case-conversion
    algorithm.
\item Although not all of Common Crawl is in valid UTF-8 or even in a known
    encoding, only words in valid UTF-8 are written to the file.
\end{itemize}

When evaluating GloVe, it is important to keep these standardizations in mind
and also apply them to the word-similarity datasets, such as lowercasing the
word ``OPEC'' in {\sc wordsim-353}.

The 840B-token dataset is not lowercased, as described in
\cite{pennington2014glove}, but it is also not all in UTF-8. It is possible
that the tokens were written to the file exactly as they were found in
Common Crawl, as bytes of ``encoding salad'', as one might expect from
scraping arbitrary Web pages. They are still tokenized in a way that is
reasonable for English.

\subsubsection{Aligning ConceptNet and Glove}

ConceptNet's standardization process is stronger than GloVe's, so we align them
by applying the ConceptNet standardizations to the GloVe labels as well, turning
its tokens into ConceptNet URIs. The GloVe tokens are assumed to be in English.

This has the effect that many separate rows of the GloVe matrix get the same
label, such as when the same word appears with a different capitalization. A
few options for dealing with these merged labels come to mind:

\begin{enumerate}
\item Keep only the row with the highest frequency in GloVe.
\item Average the rows together.
\item Take a weighted average that favors higher-frequency rows.
\end{enumerate}

The weighted average turns out to be the best approach, as our evaluation will
show. The multiple rows contain valuable data that should not simply be
discarded, but lower-frequency rows tend to have lower-quality data.

The word frequency data is not distributed with the GloVe matrix, but
the rows are clearly in descending order of frequency. We approximate the
frequency distribution by assuming that the tokens are distributed according to
Zipf's law \cite{zipf1949human}: the $n$th token in rank order has a frequency
proportional to $1/n$.

In order to evaluate the combined word vectors, it is necessary for us to apply
the same standardizations to the evaluation data. A drawback to this
standardization process is that we will not be able to evaluate the result on
Mikolov et al.'s syntactic analogy task \cite{mikolov2013word2vec}, as many of
the analogies would become trivial analogies of the form
``A is to A as B is to B''.

\section{Evaluation}

\subsection{Word similarity data sets}

We evaluate our model's performance at identifying similar words using a
variety of word-similarity gold standards:

\begin{itemize}
\item MEN-3000 \cite{bruni2014men}, crowd-sourced similarity judgments for 3000
    word pairs.
\item The Stanford Rare Words (RW) dataset \cite{luong2013rw}, crowd-sourced
    similarity judgments for 2034 word pairs, with a bias toward uncommon words.
\item SCWS*. SCWS \cite{huang2012scws} contains crowd-sourced similarity
    judgments for 2003 word pairs in the context of sentences, while SCWS* is
    the modification of it described by \citeauthor{luong2013rw}
    \shortcite{luong2013rw} to disregard the context in order to evaluate
    systems that cannot make use of context. SCWS* discards the pairs of
    identical words that are only distinguished by context, leaving 1759 pairs.
\item WordSim-353 \cite{finkelstein2001ws}, a widely-used corpus of similarity
    judgments for 353 word pairs.
\item RG-65 \cite{rubinstein1965rg}, a classic corpus of similarity judgments
    for 65 word pairs.
\item MC-30 \cite{miller1991mc}, similarity judgments for 30 word pairs.
\end{itemize}

\begin{table*}[t]
\centering
\input{../code/build-data/evaluations.latex}
\caption{
    Results on the word similarity task, shown as the Spearman rank correlation
    ($\rho$) between the learned embeddings and various human-annotated corpora.
    Here we compare the performance of GloVe in its original form, GloVe with
    its labels standardized and merged like ConceptNet, and GloVe combined with
    ConceptNet by retrofitting, for various normalizations of the GloVe vectors.
}
\label{eval-retro-standardize}
\end{table*}

In striving to maximize an evaluation metric, it is important to hold out some
data, to avoid overfitting to the test data by modifying the algorithm and its
parameters. The metrics we focused on improving were our rank correlation with
MEN-3000, which emphasizes having high-quality representations of common words,
and RW, which emphasizes having a broad vocabulary.

MEN-3000
comes with a training/test split, where 1000 of the 3000 word pairs are held
out for testing. We applied a similar split to RW, setting aside a sample
of $1/3$ of its word pairs for testing. The test set was not used in our
evaluations until the final run before submitting this article. While
Table~\ref{eval-retro-standardize} shows our correlation with the development
and test data combined, Table~\ref{eval-heldout-test} compares our performance
on development and test data.

% TODO: we have to actually do the above before we submit

% TODO: lots of stuff
% TODO: evaluate other row-combining metrics
% TODO: retrofit PPDB instead of ConceptNet and compare

\section{Discussion}

When applying ConceptNet's standardization procedure to the labels of GloVe's
840B-token dataset, we achieved an unexpected benefit of cleaning up the data
so it performs much better on the rare words evaluation.

When we run our evaluation without modifying the labels on the GloVe 42B-token
embeddings, we reproduce the results published in \cite{pennington2014glove}.
This same evaluation, when run on the 840B-token embeddings, produces
surprisingly poor results, implying that some of these rare words appear in
forms that have low-quality embeddings. When we standardize the GloVe labels
and combine them using the Zipf estimate, however, the result outperforms the
42B-token embeddings.

We conclude from this that it is important to combine the information learned
from multiple forms of a word, including differences in inflection and
capitalization, instead of relying on a single word form to have the correct
vector. However, even if too many word forms were distingushed in the GloVe
learning process, re-combining the word forms after the fact is sufficient to
repair the data.

\subsection{Analogies are not our goal}
\label{analogies-meh}

The analogy evaluation of \cite{mikolov2013word2vec} is an interesting
emergent property to demonstrate of a system that has no syntactic or
relational knowledge built into it. However, at the point where the system
{\em does} have more knowledge built into it, it becomes somewhat of
a meaningless exercise.

The syntactic analogies \ldots %TODO

\subsection{The limits of similarity judgments}

\citeauthor{bruni2014men} \shortcite{bruni2014men} describe a substitute for
inter-annotator agreement on their MEN-3000 dataset:

\begin{quote}
The Spearman correlation of the two authors is at 0.68, the correlation of their
average ratings with the MEN scores is at 0.84. On the one hand, this high
correlation suggests that MEN contains meaningful semantic ratings. On the
other, it can also be taken as an upper bound on what computational models can
realistically achieve when simulating the human MEN judgments.
\end{quote}

Our model achieves a MEN score of \scoreMEN{}, after re-introducing the
held-out test data, putting its performance above that upper bound. In many
circumstances, this would be an indication that the model has overfit to the
MEN data. However, we have tried to eliminate the possibility of overfitting.
The MEN data is never used as an input to the system; we did not use the
held-out data when setting parameters; and the same system that performs above
the claimed upper bound of MEN also performs the highest on all the other data
sets we tried.

We postulate instead that similarity evaluations become easier to match
computationally when averaged over the judgments of many people. The two
authors agreed more with the average of the crowd than they did with each other;
if more people's judgments had been averaged in with the authors, perhaps the
correlation would have been even higher. We suspect that the true upper bound
of the MEN evaluation is higher than claimed.

\begin{CJK*}{UTF8}{min}
\bibliography{wordsim_paper}
\end{CJK*}

\end{document}
