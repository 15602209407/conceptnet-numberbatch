\def\year{2015}
\def\thetitle{Improving Word Vectors by Adding Broad, Structured Knowledge}

% Update this when our score changes. Maybe we can even automate this.
\def\scoreRW{.589}
\def\scoreMEN{.860}

\documentclass[letterpaper]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{CJKutf8}
\usepackage{url}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title \thetitle
/Author Rob Speer, Joshua Chin, and Catherine Havasi}
\setcounter{secnumdepth}{0}
\bibliographystyle{acl2012}

\title{\thetitle}
%\author{Robert Speer\\
%    Luminoso Technologies, Inc.\\
%    675 Massachusetts Ave.\\
%    Cambridge, MA 02139\\
%    \texttt{rspeer@luminoso.com}
%\And
%    Joshua Chin\\
%    {\em academic address goes here}
%\And
%    Catherine Havasi\\
%    Luminoso Technologies, Inc.\\
%    675 Massachusetts Ave.\\
%    Cambridge, MA 02139\\
%    \texttt{havasi@luminoso.com}
%}

% TODO: look for instances of the word 'similarity' and decide whether we mean
% 'relatedness'

\begin{document}
\maketitle
\begin{abstract}
Embeddings of natural-language words in a vector space are useful for a variety
of tasks in semantics, and these embeddings are often evaluated on their ability
to rank similar words. In this paper, we show the effectiveness of combining
existing embeddings learned by GloVe with structured knowledge from the semantic
network ConceptNet 5, taking care to merge them into a common word
representation. The resulting vector space has a larger vocabulary than either
source, works on non-English words and multiple-word phrases, and achieves
state-of-the-art performance on word similarity evaluations, including an
unprecedented score of $\rho = \scoreRW{}$ on the Stanford rare-words
evaluation.
\end{abstract}

\section{Introduction}
Embeddings of natural-language terms in a vector space are useful for a variety
of tasks in semantics. The goal is to represent words or phrases in a vector
space with hundreds of dimensions, such that the cosine similarity between
these vectors is higher when the words are more similar. This kind of vector
space has been useful in applications such as information retrieval
applications such as search and topic detection, dating back to the
introduction of latent semantic analysis \cite{deerwester1990lsa}. In recent
years, there has been a surge of interest in natural-language embeddings, as
machine-learning techniques such as \newcite{mikolov2013word2vec}'s word2vec
began to show dramatic improvements.

A recent result from \newcite{faruqui2014retrofitting} introduced a technique
known as ``retrofitting'', which combines embeddings learned from the
distributional semantics of unstructured text with a source of structured
connections between words, achieving performance on word-similarity evaluations
that neither source would achieve on its own.

We have expanded the retrofitting technique to operate on the union of two
vocabularies, rather than the intersection. Combining this technique with some
subtle but important normalizations to the input data, we achieve state-of-the-art
performance when combining the distributional
embeddings learned by GloVe \cite{pennington2014glove} from the Common Crawl
\cite{TODOcrawl} with the structured word and phrase associations in ConceptNet
5 \cite{speer2012conceptnet}. These new embeddings outperform many other
recently-published methods, when compared to various human judgments of word
similarity on both common and rare words.

In all, we take a three-pronged approach to improving word similarity. We will
show that these methods all improve performance on word-similarity evaluations,
even when applied separately, and that the best improvement comes from applying
all of them at once:

\begin{itemize}
\item Normalizing the features of the GloVe matrix with $L_1$ instead of $L_2$
    normalization, providing a bonus to more selective features
\item Transforming the matrix labels with a text ``standardization'' function,
    unifying variations and inflections of the same word
\item Applying a variation of retrofitting to introduce a broad variety of
    structured semantic knowledge from ConceptNet 5.4
\end{itemize}

The family of techniques that includes word2vec and GloVe is sometimes popularly
referred to as ``deep learning'', but this seems to be a buzzword with a vague
referent. The multi-layered neural networks that ``deep learning'' was
originally intended to refer to \cite{hinton2006deep} tend to be absent in these
successful semantic representations.

For the purpose of semantics, what seems to be more important thus far than
multiple levels of learning is the ability to efficiently take in lots of input
from a variety of sources. Combining multiple forms of knowledge to create the
embeddings furthers this goal. Perhaps it would be clearer to refer to this
family of techniques as ``wide learning'' instead.

\section{Background}

\subsection{GloVe}

GloVe \cite{pennington2014glove} is a an unsupervised learning algorithm that
trains vector representations of words. The algorithm operates on a global
word-word co-occurrence matrix $C$. Let $v$ denote the size of the vocabulary.
It trains $W$ and $\tilde{W}$, both $v \times n$ matrices, that
denote the word vectors and context vectors respectively, and $b$ and
$\tilde{b}$, both $v$-length vectors representing the word and context biases
respectively. It aims to minimize

$$
\sum_{i=1,j=1}^v
  f \left( C_{ij} \right)
  \left( W_i^T \tilde{W}_j + b_i + \tilde{b}_j - \log{C_{ij}} \right)^2
$$

where $f$ acts as a weighting function of the form

$$
f \left( x \right) =
  \begin{cases}
    \left( x / x_{max} \right)^\alpha & \text{if $x < x_{max}$ } \\
    1 & otherwise
  \end{cases}
$$

The GloVe authors experimentally determined $x_{max}=100$ and $\alpha=0.75$.

Learning processes such as GloVe produce results that can conveniently be
re-used in other projects, because they output a labeled matrix of the learned
embeddings of each word in the vocabulary. The embeddings that GloVe learns
from data sources such as the Common Crawl are distributed on the GloVe web
page\footnote{\url{http://nlp.stanford.edu/projects/glove/}}.

\newcite{faruqui2014retrofitting}
introduced the ``retrofitting'' procedure, which adjusts such dense matrices of
embeddings based on external knowledge from a sparse semantic network.
They used PPDB \cite{ganitkevitch2013ppdb} as
the external source of semantic knowledge. We found using ConceptNet as the
external knowledge source to be more effective, and that slight further
improvements could be achieved on some evaluations by adding both
ConceptNet and PPDB, as shown in Table~\ref{eval-retro-standardize}.

\subsection{ConceptNet}
ConceptNet 5 \cite{speer2012conceptnet} is a semantic network of terms
connected by labeled relations. Its terms are words or multiple-word phrases
in a variety of natural language. For continuity with previous work,
these terms are often referred to as {\em concepts}.

ConceptNet originated as a machine-parsed version of the early crowd-sourcing
project called Open Mind Common Sense (OMCS) \cite{singh2002omcs}, and has expanded
to include several other data sources, both crowd-sourced and expert-created.
The data sources included in ConceptNet 5.4 are:

\begin{itemize}
\item Knowledge collected as English sentences on the original OMCS website,
    and later parsed with a pattern-matching parser
\item Sister projects to OMCS that collected similar sentences in Portuguese
    \cite{anacleto2006portuguese},
    Dutch \cite{eckhardt2008kid}, and some Korean and Japanese
    \cite{chung2006globalmind}
\item ``Games with a purpose'' that collect relational knowledge, such as
    Verbosity \cite{vonahn2006verbosity} in English, {\em nadya.jp}
    \cite{nakahara2011nadya} in Japanese, and the PTT pet game \cite{kuo2009petgame}
    in Taiwanese Chinese
\item WordNet 3.0 \cite{miller1998wordnet}
\item A parsed version of Wiktionary's entries whose target language is English
    or German \cite{wiktionary2014en} \cite{wiktionary2014de}, whose parser is
    developed as part of the ConceptNet 5 codebase
\item JMDict \cite{breen2004jmdict}, a Japanese multilingual dictionary
\item OpenCyc \cite{matuszek2006cyc} as represented in Umbel \cite{bergman2008umbel}
\end{itemize}

ConceptNet 5.4 also includes a mapping of DBPedia \cite{auer2007dbpedia} into
its term space, but it is not used for word associations, so its presence does
not affect the results in this paper.

% TODO: ConceptNet image


\section{Methods}

Here, we create a matrix that assigns embeddings in a 300-dimensional vector
space to terms by combining data from ConceptNet and GloVe. The resulting word
embeddings incorporate the strengths of both ConceptNet and GloVe, and extend
their vocabulary to the union of ConceptNet and GloVe, allowing embeddings that
were originally only trained on English to be extended to other languages.

\subsection{Standardizing Text}

As \newcite{levy2015embeddings} note,
``[\ldots] much of the performance gains of word embeddings are due to certain
system design choices and hyperparameter optimizations, rather than the
embedding algorithms themselves.'' While it is presented as a negative result,
this simply emphasizes the importance of these system design choices.

Indeed, we have found that choices about how to handle text have a significant
impact on evaluation results. Additionally, we can only properly combine two
resources with a method such as retrofitting if their string representations
are comparable to each other.

The operations we apply to text would usually be called ``normalization'',
but that word also refers to what we'll be doing to our vectors, so both
here and in the code we call it ``standardization'' instead.

\subsubsection{How ConceptNet is Standardized}

Here are the standardizations applied to all terms in ConceptNet 5.4, as
implemented in the {\tt conceptnet5.nodes.normalized\_concept\_uri} function
of ConceptNet 5.4:

\begin{itemize}
\item The words and phrases that the data sources provide are first run
    through the {\tt ftfy.fix\_text} function, in version 4.0 of the Python
    module {\tt ftfy}.\footnote{
        \url{http://github.com/LuminosoInsight/python-ftfy}
    } This corrects ``mojibake'' (a Unicode glitch that arises when reading
    text in the wrong encoding), as well as applying NFC normalization.\footnote{
        For example, ConceptNet considers ``ni\~{n}o'' to be a single
        label, regardless of whether it is spelled as the four codepoints
        {\tt n i \~{n} o}, the five codepoints {\tt n i n \textasciitilde{} o}
        where the fourth is a combining character,
        or the five corrupted codepoints {\tt n i \~{A} \textpm{} o}.
    }
\item The text is tokenized using a regular expression that splits it at
    characters whose major Unicode category is Z (whitespace), P (punctuation),
    S (symbols), or C (control characters).
\item If the text is English, each token is lemmatized using a modification of
    WordNet's Morphy algorithm, which appears in the {\tt conceptnet5.language.english}
    module of ConceptNet 5.4's Python code. This converts inflected words such
    as ``creating'' to their root words that appear in WordNet, such as
    ``create''.
    % Because most data sources are not tagged with parts of speech,
    % it uses heuristics to choose which part of speech to lemmatize the word as,
    % plus exceptions that allow it to choose more common roots when Morphy would
    % give multiple options.
\item If the text is English, a very small list of stopwords (``a'', ``an'',
    and ``the'') are removed, as well as ``to'' if it is the first word.
\item Spaces and slashes are replaced by underscores.
\item The text is lowercased according to the Unicode case-mapping algorithm
    that is implemented in Python 3.4.
\item The resulting standardized text is tagged with its language and turned
    into a URI: the phrase ``giving an example'', for example, becomes
    {\tt /c/en/give\_example}.
\end{itemize}

The resulting URIs are not entirely meant for human consumption, and are
sometimes not grammatically accurate -- the term ``ground zero'' becomes
{\tt /c/en/grind\_zero}, for example -- but they unify surface texts that may
appear in slightly different forms in different resources.

\subsubsection{How GloVe is Standardized}

The 42-billion-token version of GloVe, whose results are published in
\cite{pennington2014glove}, has applied some simple standardizations
to the text it extracts from Common Crawl:

\begin{itemize}
\item The text is tokenized with the Stanford parser. Only single tokens are
    used in GloVe.
\item The words are lowercased using a Unicode case-conversion
    algorithm.
\item Although not all of Common Crawl is in valid UTF-8 or even in a known
    encoding, only words in valid UTF-8 are written to the file.
\end{itemize}

When evaluating GloVe, it is important to keep these standardizations in mind
and also apply them to the word-similarity datasets, such as lowercasing the
word ``OPEC'' in WordSim-353.

The 840B-token dataset is not lowercased, as described in
\cite{pennington2014glove}, but it is also not all in UTF-8. It is possible
that the tokens were written to the file exactly as they were found in
Common Crawl, as bytes of ``encoding salad'', as one might expect from
scraping arbitrary Web pages. They are still tokenized in a way that is
reasonable for English.

\subsubsection{Aligning ConceptNet and GloVe}

ConceptNet's standardization process is stronger than GloVe's, so we align them
by applying the ConceptNet standardizations to the GloVe labels as well, turning
its tokens into ConceptNet URIs. The GloVe tokens are assumed to be in English.

This has the effect that many separate rows of the GloVe matrix get the same
label, such as when the same word appears with a different capitalization. A
few options for dealing with these merged labels come to mind:

\begin{enumerate}
\item Keep only the row with the highest frequency in GloVe.
\item Average the rows together.
\item Take a weighted average that favors higher-frequency rows.
\end{enumerate}

The weighted average turns out to be the best approach, as our evaluation will
show. The multiple rows contain valuable data that should not simply be
discarded, but lower-frequency rows tend to have lower-quality data.

The word frequency data is not distributed with the GloVe matrix, but
the rows are clearly in descending order of frequency. We approximate the
frequency distribution by assuming that the tokens are distributed according to
Zipf's law \cite{zipf1949human}: the $n$th token in rank order has a frequency
proportional to $1/n$.

In order to evaluate the combined word vectors, it is necessary for us to apply
the same standardizations to the evaluation data. A drawback to this
standardization process is that we will not be able to evaluate the result on
Mikolov et al.'s syntactic analogy task \cite{mikolov2013word2vec}, as many of
the analogies would become trivial analogies of the form
``A is to A as B is to B''.

Due to a quirk in some of the evaluation data, we must apply one more
standardization to all labels, even though it is not necessary in ConceptNet
or in GloVe. The Spanish translations of word-similarity evaluations
\cite{TODOspanish} turn out to be written without any accents on the letters,
such as ``telefono'' instead of ``tel\'{e}fono''.
In order to recognize these as the correct words instead of out-of-vocabulary
words, we need to re-standardize all Spanish terms in ConceptNet to omit all
of the accents.

% Let's have a bit of an interlude for tables. They don't belong with this
% section but they need to be able to appear early enough in the paper despite
% LaTeX's constraints.

\begin{table*}[t]
\centering
\begin{tabular}{lllllrrrrrr}
\toprule
Note &Embeddings   & Norm  & Text std. & Retrofit &       RW & MEN-3000 &    WS-353 &      SCWS &    RG-65 &    MC-30 \\
\midrule
$[1]$&Levy SVD     & ---   & ---       & ---      &     .514 &     .778 &           &           &          &          \\
\midrule
     &GloVe 42B    & ---   & ---       & ---      &     .391 &     .734 &      .632 &      .440 &     .817 &     .777 \\
     &GloVe 42B    & ---   & CN5.4     & ---      &     .373 &     .763 &      .646 &      .444 &     .810 &     .762 \\
$[2]$&GloVe 42B    & $L_2$ & ---       & ---      &     .489 &     .813 &      .759 &      .595 &     .829 &     .836 \\
     &GloVe 42B    & $L_2$ & CN5.4     & ---      &     .494 &     .809 &      .765 &      .587 &     .779 &     .815 \\
     &GloVe 42B    & $L_1$ & ---       & ---      &     .497 &     .817 &      .766 &      .606 &     .826 &     .829 \\
     &GloVe 42B    & $L_1$ & CN5.4     & ---      &     .518 &     .834 &      .794 &      .619 &     .814 &     .828 \\
\midrule
     &GloVe 840B   & ---   & ---       & ---      &     .106 &     .710 &      .627 &      .441 &     .648 &     .696 \\
     &GloVe 840B   & ---   & CN5.4     & ---      &     .468 &     .806 &      .735 &      .552 &     .775 &     .787 \\
     &GloVe 840B   & $L_2$ & ---       & ---      &     .116 &     .750 &      .664 &      .496 &     .652 &     .666 \\
     &GloVe 840B   & $L_2$ & CN5.4     & ---      &     .519 &     .817 &      .760 &      .564 &     .717 &     .789 \\
     &GloVe 840B   & $L_1$ & ---       & ---      &     .117 &     .754 &      .667 &      .500 &     .653 &     .682 \\
     &GloVe 840B   & $L_1$ & CN5.4     & ---      &     .536 &     .841 &      .798 &      .615 &     .774 &     .798 \\
\midrule
     &GloVe 840B   & ---   & CN5.4     & PPDB     &     .468 &     .813 &      .716 &      .598 &     .815 &     .815 \\
     &GloVe 840B   & ---   & CN5.4     & CN5.4    &     .516 &     .826 &      .734 &      .602 &     .842 &     .810 \\
     &GloVe 840B   & ---   & CN5.4     & Both     &     .520 &     .827 &      .731 &      .604 &     .838 &     .811 \\
     &GloVe 840B   & $L_2$ & CN5.4     & PPDB     &     .555 &     .830 &      .780 &      .633 &     .788 &     .819 \\
     &GloVe 840B   & $L_2$ & CN5.4     & CN5.4    &     .575 &     .841 &      .794 &      .631 &     .805 &     .836 \\
     &GloVe 840B   & $L_2$ & CN5.4     & Both     &     .580 &     .841 &      .794 &      .634 &     .801 &     .829 \\
     &GloVe 840B   & $L_1$ & CN5.4     & PPDB     &     .565 &     .852 &      .806 & {\bf .674}&     .821 &     .824 \\
$[3]$&GloVe 840B   & $L_1$ & CN5.4     & CN5.4    &     .587 &{\bf .858}& {\bf .818}&      .668 &{\bf .852}&{\bf .845}\\
     &GloVe 840B   & $L_1$ & CN5.4     & Both     &{\bf .591}&{\bf .858}&      .817 &      .671 &     .846 &     .842 \\
\bottomrule
\end{tabular}

\caption{
    Results on the word similarity task, shown as the Spearman rank correlation
    ($\rho$) between the learned embeddings and various human-annotated corpora.
    ``Norm'' indicates the norm applied to the columns of GloVe.
    ``Text std.'' indicates whether labels are left in their original form or
    standardized according to ConceptNet 5.4. ``Retrofit'' indicates which data
    is added using retrofitting.\\
    {\bf Notes}: $[1]$ Results from \newcite{levy2015embeddings}, for comparison. $[2]$ These results correspond to those published by \newcite{pennington2014glove}.
    $[3]$ These results are the ones we will use for further experimentation, such as in Table~\ref{eval-variations}.
}
\label{eval-retro-standardize}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{lllllrrrr}
\toprule
Note &Embeddings   & Norm  & Text std. & Retrofit & WS-353 [es] & MC-30 [es] & RG-65 [de] & RG-65 [fr] \\
\midrule
$[1]$&Skip-grams   & $L_2$ & ---       & UWN      &             &       .591 &       .603 &       .606 \\
     &GloVe 840B   & $L_1$ & CN5.4     & CN5.4    &        .444 &       .765 &       .673 &       .781 \\
\bottomrule
\end{tabular}

\caption{
    Multilingual evaluation results.
    {\bf Note}: $[1]$ These are the published results of retrofitting
    Universal WordNet onto skip-gram embeddings from Wikipedia
    \cite{faruqui2014retrofitting}, for comparison.
}
\label{eval-multilingual}
\end{table*}

% These are evaluated on the development data only. Re-running them with the
% held-out test data doesn't make temporal sense.
\begin{table*}[t]
\centering
\begin{tabular}{llrrrrrr}
\toprule
Mod. type       & Modification        &   RW &  MEN-3000 &  WS-353 &  SCWS &  RG-65 &  MC-30 \\
\midrule
Unmodified      &                     & .587 &      .858 &    .818 &  .668 &   .852 &   .845 \\
\midrule
Row-merging     & First row only      & .563 &      .827 &    .787 &  .649 &   .822 &   .794 \\
Row-merging     & Unweighted avg.     & .526 &      .844 &    .751 &  .586 &   .841 &   .836 \\
Retrofitting    & No self-loops       & .570 &      .855 &    .790 &  .676 &   .873 &   .853 \\
Sources         & Drop ConceptNet 4   & .588 &      .857 &    .816 &  .669 &   .855 &   .848 \\
Sources         & Drop JMDict         & .585 &      .858 &    .819 &  .666 &   .851 &   .840 \\
Sources         & Drop OpenCyc        & .587 &      .858 &    .819 &  .668 &   .852 &   .845 \\
Sources         & Drop Verbosity      & .587 &      .853 &    .814 &  .668 &   .852 &   .837 \\
Sources         & Drop Wiktionary     & .541 &      .865 &    .818 &  .664 &   .867 &   .852 \\
Sources         & Drop WordNet        & .586 &      .858 &    .817 &  .667 &   .852 &   .845 \\
Sources         & Wiktionary only     & .585 &      .852 &    .814 &  .665 &   .852 &   .840 \\
\bottomrule
\end{tabular}
\caption{
    The effects of various modifications to the embeddings, including
    changing the way that embeddings are merged after standardization, and
    dropping various knowledge sources from ConceptNet. ``ConceptNet 4''
    represents a combination of ported sources that are difficult to
    separate: Open Mind Common Sense, GlobalMind, and nadya.jp.
}
\label{eval-variations}
\end{table*}

\subsection{Normalization}

As briefly mentioned by \newcite{pennington2014glove}, $L_2$ normalization of
the columns of the GloVe matrix often provides a notable increase in
performance. $L_2$ normalization can be generalized to arbitrary
$L_p$ normalization by dividing a vector by its $p$-norm. The $p$-norm of an
$n$-element vector is defined by

$$ \left\|\mathbf{x}\right\|_p
  = \sum_{i=1}^n \left|x_i\right|^p$$

Normalization attempts to promote discriminative features of word vectors.
Features with high $p$-norms are less discriminative and therefore less useful.
However, while $L_2$ normalization provides an improvement over the un-normalized
vectors, it weights outliers very highly, which are the features that we want to
preserve. $L_1$ on the other hand provides an equal weighing for all inputs.
Our tests show that $L_1$ normalization provides a consisent performance
increase over $L_2$ normalization in almost all variations of GloVe.

\subsection{Retrofitting}

Retrofitting \cite{faruqui2014retrofitting} is a process of combining existing word vectors with a semantic
lexicon. While the original formulation expresses the problem in terms of a set
of edges, we have found it more convenient to express it in terms of matrices.
Let $v$ denote the size of the vocabulary, $S$ be a $v \times v$ matrix
denoting the semantic lexicon, and $W$ be a $v \times n$ matrix denoting the
original word vectors. If a word does not have an
corresponding word vector, it is assigned the zero vector. Retrofitting
generates a $W'$ that minimizes

$$
\Psi \left( W' \right) = \sum_{i=0}^v \left[
  \alpha_i \left\|  w'_i - w_i \right\| ^ 2
  + \sum_{j=0}^v s_{ij} \left\| w'_i - w'_j \right\| ^ 2
\right]
$$

where $\alpha_i$ is a column weight vector. In our tests, we let

$$
\alpha_i =
  \begin{cases}
    0 & \text{if $W^0_i = \vec{0}$} \\
    1 & \text{otherwise}
  \end{cases}
$$

Notice that $\Psi$ is convex and can be solved by a simple iterative updating
method:

$$
W^{k+1} = \left( S W^k + \alpha \odot W^0 \right)
\oslash \left( \vec{1} + \alpha \right)
$$

where $\oslash$ denotes row-wise division, $\odot$ denotes row-wise
multiplication, $\vec{1}$ is a vector of all ones and $W^0$ are the original
word vectors.

\subsubsection{Row-Normalized Retrofitting}

The magnitude of a vector acts as an implicit $\alpha$ in retrofitting.
The $L_2$ norms of the GloVe vectors span several orders of magnitude.
Nonsense associations within the semantic lexicon between a vector with a large
$L_2$ norm and a vector with small $L_2$ norm could remove much of the semantic
information contained within the small vector. Therefore, we $L_2$-normalize
the rows of the word vectors both before and during the retrofitting process,
which yields an increase in performance.

\subsection{Self-Loops}
%TODO Check if Self Loops actually help

\subsection{ConceptNet as an Association Matrix}

In order to apply the retrofitting method, we need to consider the data in
ConceptNet as a sparse, symmetric matrix of associations between terms. What
ConceptNet provides is more complex than that, as it connects terms with a
variety of not-necessarily-symmetric, labeled relations.

\newcite{havasi2010color} introduced a vector space embedding of ConceptNet,
``spectral association'', that disregarded the relation labels for the purpose
of measuring the relatedness of terms. Previous embeddings of ConceptNet, such
as that of \newcite{speer2008analogyspace}, preserved the relations but were
suited mostly for direct similarity and inference, not for relatedness. Because
most evaluation data for word similarity is also evaluating relatedness, unless
there has been a specific effort to separate them \cite{agirre2009similarity},
we erase the labels as in spectral association.

Negated relations and antonym relations, such as the relations expressed by
``a person does not have a tail'' and ``hot is the opposite of cold'', are
typically handled specially; a detail of spectral association is that negated
relations were assigned negative values in the matrix. Intuitively, negated
relations between two terms should not increase their relatedness. Perhaps
these relations should in fact decrease relatedness -- while ``hot'' and
``cold'' are similar in many ways, people providing judgments of relatedness
are likely to consider them less related because of the obvious contrast
between them.

However, association matrices are better behaved, especially under
normalization, when they contain no negative entries. In our method, we simply
omit all instances of negative and antonym relations in ConceptNet.

Each remaining assertion in ConceptNet corresponds to two entries in a sparse
association matrix $S$. ConceptNet assigns a confidence score, or weight, to each
assertion. An assertion that relates term $i$ to term $j$ with weight $w$ will
contribute $w$ to the value of $S_{ij}$ and $S_{ji}$. If another assertion relates
the same terms with a different relation, it will add to that value.

%TODO: describe the scaling of datasets

\section{Evaluation}

\subsection{Word Similarity Datasets}

We evaluate our model's performance at identifying similar words using a
variety of word-similarity gold standards:

\begin{itemize}
\item MEN-3000 \cite{bruni2014men}, crowd-sourced similarity judgments for 3000
    word pairs.
\item The Stanford Rare Words (RW) dataset \cite{luong2013rw}, crowd-sourced
    similarity judgments for 2034 word pairs, with a bias toward uncommon words.
\item SCWS*. SCWS \cite{huang2012scws} contains crowd-sourced similarity
    judgments for 2003 word pairs in the context of sentences, while SCWS* is
    the modification of it described by \newcite{luong2013rw}
    to disregard the context in order to evaluate
    systems that cannot make use of context. SCWS* discards the pairs of
    identical words that are only distinguished by context, leaving 1759 pairs.
\item WordSim-353 \cite{finkelstein2001ws}, a widely-used corpus of similarity
    judgments for 353 word pairs.
\item RG-65 \cite{rubinstein1965rg}, a classic corpus of similarity judgments
    for 65 word pairs.
\item MC-30 \cite{miller1991mc}, similarity judgments for 30 word pairs.
\end{itemize}

% TODO: describe foreign language datasets

In striving to maximize an evaluation metric, it is important to hold out some
data, to avoid overfitting to the test data by modifying the algorithm and its
parameters. The metrics we focused on improving were our rank correlation with
MEN-3000, which emphasizes having high-quality representations of common words,
and RW, which emphasizes having a broad vocabulary.

MEN-3000
comes with a training/test split, where 1000 of the 3000 word pairs are held
out for testing. We applied a similar split to RW, setting aside a sample
of $1/3$ of its word pairs for testing. The test set was not used in our
evaluations until the final run before submitting this article. While
Table~\ref{eval-retro-standardize} shows our correlation with the development
and test data combined, Table~\ref{eval-heldout-test} compares our performance
on development and test data.

% TODO: we have to actually do the above before we submit

% TODO: compare PPDB to ConceptNet
% TODO: evaluate other row-combining metrics

\section{Discussion}

When applying ConceptNet's standardization procedure to the labels of GloVe's
840B-token dataset, we achieved an unexpected benefit of cleaning up the data
so it performs much better on the rare words evaluation.

When we run our evaluation without modifying the labels on the GloVe 42B-token
embeddings, we reproduce the results published in \cite{pennington2014glove}.
This same evaluation, when run on the 840B-token embeddings, produces
surprisingly poor results, implying that some of these rare words appear in
forms that have low-quality embeddings. When we standardize the GloVe labels
and combine them using the Zipf estimate, however, the result outperforms the
42B-token embeddings.

We conclude from this that it is important to combine the information learned
from multiple forms of a word, including differences in inflection and
capitalization, instead of relying on a single word form to have the correct
vector. However, even if too many word forms were distingushed in the GloVe
learning process, re-combining the word forms after the fact is sufficient to
repair the data.

%\subsection{Analogies Are Not Our Goal}
%\label{analogies-meh}
%
%The analogy evaluation of \cite{mikolov2013word2vec} is an interesting
%emergent property to demonstrate of a system that has no syntactic or
%relational knowledge built into it. However, at the point where the system
%{\em does} have more knowledge built into it, it becomes somewhat of
%a meaningless exercise.
%
%The syntactic analogies \ldots %TODO

\subsection{The Limits of Similarity Judgments}

\newcite{bruni2014men} describe a substitute for
inter-annotator agreement on their MEN-3000 dataset:

\begin{quote}
``The Spearman correlation of the two authors is at 0.68, the correlation of their
average ratings with the MEN scores is at 0.84. On the one hand, this high
correlation suggests that MEN contains meaningful semantic ratings. On the
other, it can also be taken as an upper bound on what computational models can
realistically achieve when simulating the human MEN judgments.''
\end{quote}

Our model achieves a MEN score of \scoreMEN{} on the
held-out test data, putting its performance above that upper bound. In many
circumstances, this would be an indication that the model has overfit to the
MEN data. However, we have tried to eliminate the possibility of overfitting.
The MEN data is never used as an input to the system; the same system also
achieves good results on all other data sets; and we did not use or even
look at the held-out data when setting parameters. Strikingly, the evaluation
score on MEN goes {\em up} when the held-out data is introduced.

We postulate instead that similarity evaluations become easier to match
computationally when averaged over the judgments of many people. Notice, for
example, that the two MEN-3000 authors who provided their own ratings
agreed more with the average of the crowd than they did with each other.
if more people's judgments had been averaged in with the authors, perhaps the
correlation would have been even higher. We suspect that the true upper bound
of the MEN evaluation is higher than claimed.

\begin{CJK*}{UTF8}{min}
\bibliography{wordsim_paper}
\end{CJK*}

\end{document}
