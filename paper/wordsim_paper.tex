\def\year{2015}
\def\thetitle{An Ensemble Method to Produce High-Quality Word Embeddings}

% Update this when our score changes.
\def\scoreRW{.584}
\def\scoreMEN{.860}

\documentclass[11pt,letterpaper]{article}
% Packages required by the paper format
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{booktabs}

%\naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

% Packages we need
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}
\usepackage{CJKutf8}
\usepackage{url}
%\setcounter{secnumdepth}{0}
\bibliographystyle{naaclhlt2016}

\title{\thetitle}
\author{Robert Speer\\
    Luminoso Technologies, Inc.\\
    675 Massachusetts Ave.\\
    Cambridge, MA 02139\\
    \texttt{rspeer@luminoso.com}
\And
    Joshua Chin\\
    Union College\\
    807 Union St.\\
    Schenectady, NY 12308\\
    \texttt{joshuarchin@gmail.com}
}

\date{}

\begin{document}

\maketitle
\begin{abstract}

A currently successful approach to computational semantics is to represent
words as embeddings in a machine-learned vector space.  n this paper, we
present an ensemble method that combines embeddings produced by GloVe
\cite{pennington2014glove} and word2vec \cite{mikolov2013word2vec} with
structured knowledge from the semantic network ConceptNet 5
\cite{speer2012conceptnet}, merging their information into a common
representation with a large, multilingual vocabulary. The embeddings it
produces achieve state-of-the-art performance on many word similarity
evaluations. Its score of $\rho = \scoreRW{}$ on an evaluation of rare words
\cite{luong2013rw} is 13.6\% higher than the previous best known system.
% FIXME: update for final score and data sources

\end{abstract}

\section{Introduction}

Vector space models are an effective way to express the meanings of
natural-language terms in a computational system. These models are created
using machine-learning techniques that represent words or phrases as
vectors in a high dimensional space, such that the cosine similarity of any two
terms corresponds to their semantic similarity.

These vectors, referred to as
the {\em embeddings} of the terms in the vector space, can be compared directly
to measure the similarity of two terms (as the cosine similarity of their
respective vectors), and can also be used as an input to further steps of
machine learning. When algorithms expect dense vectors as input, embeddings
provide a representation that is both more compact and more informative than the
``one-hot'' representation in which every term in the vocabulary gets its own
dimension.

This kind of vector space has been used in applications such as search, topic
detection, and text classification, dating back to the introduction of latent
semantic analysis \cite{deerwester1990indexing}.  In recent years, there has
been a surge of interest in natural-language embeddings, as machine-learning
techniques such as \newcite{mikolov2013word2vec}'s word2vec and
\newcite{pennington2014glove}'s GloVe have begun to show dramatic improvements.
Word embeddings are often suggested as an initialization for more complex
methods, such as the sentence encodings of \newcite{kiras2015skip}.

\newcite{faruqui2015retrofitting} introduced a technique
known as ``retrofitting'', which combines embeddings learned from the
distributional semantics of unstructured text with a source of structured
connections between words. The combined embedding achieves performance on
word-similarity evaluations superior to either source individually.

Here, we build on the retrofitting process to produce a high-quality space of
word embeddings. We extend existing techniques in the following ways:

\begin{itemize}
\item We include ConceptNet, a Linked Open Data semantic network that expresses
many kinds of relationships between words in many languages, as a source of
structured connections between words.
\item We modify the retrofitting algorithm, making it not depend on the row
order of its input matrix, and allowing it to propagate over the union of the
vocabularies. We call this procedure ``expanded retrofitting''.
\item We align English terms from different sources using a lemmatizer and a
heuristic for merging together multiple term vectors.
\item We fill gaps when aligning the two distributional-semantics sources
(GloVe and word2vec) using a locally linear interpolation.
\item We re-scale the distributional-semantics features using L1 normalization.
\end{itemize}

When we use this process to combine word2vec, GloVe, ConceptNet, and PPDB
\cite{ganitkevitch2013ppdb}, this process produces a space of term embeddings
that achieves state-of-the-art performance on word-similarity
evaluations\footnote{Some methods distinguish word similarity from word
{\em relatedness}. ``Coffee'' and ``mug'', for example, are quite related,
but not actually similar because coffee is not {\em like} a mug. In this paper,
however, we conflate similarity and relatedness into the same metric, as most
evaluations do.}
over both common and rare words.


\subsection{Related work}

% TODO: cite Turney 2010 survey
% TODO: talk about Zou and multilingual embeddings
% TODO: talk about Bian and word senses


\section{Knowledge sources}

\subsection{ConceptNet and PPDB}
ConceptNet \cite{speer2012conceptnet} is a semantic network of terms
connected by labeled relations. Its terms are words or multiple-word phrases
in a variety of natural languages. For continuity with previous work,
these terms are often referred to as {\em concepts}.

ConceptNet originated as a machine-parsed version of the early crowd-sourcing
project called Open Mind Common Sense (OMCS) \cite{singh2002omcs}, and has expanded
to include several other data sources, both crowd-sourced and expert-created,
by unifying their vocabularies into a single representation. ConceptNet now includes
representations of WordNet \cite{miller1998wordnet}, Wiktionary \cite{wiktionary2014en},
and JMDict \cite{breen2004jmdict}, as well as data from ``games with a purpose'' in
multiple languages \cite{vonahn2006verbosity} \cite{nakahara2011nadya}
\cite{kuo2009petgame}. We choose not to include ConceptNet's alignment to DBPedia
here, as DBPedia focuses on relations between specific named entities, which do not help
with general word similarity.\footnote{
    Given different goals -- such as achieving a high score on
    \newcite{mikolov2013word2vec}'s analogy evaluation that tests relations
    such as ``{\em A} is the CEO of company {\em B}'' -- including an appropriate
    representation of DBPedia would of course be helpful.
}

PPDB \cite{ganitkevitch2013ppdb} is another resource that is useful for
learning about word similarity, providing different information from
ConceptNet. It lists pairs of words that are translated to the same word in parallel
corpora, particularly in documents of the European parliament.

\subsection{word2vec and GloVe}

word2vec and GloVe are two current systems that learn vector representations
of words according to their distributional semantics. Given a large text corpus,
they produce vectors representing similarities in how the words co-occur with
other words.

\newcite{mikolov2013word2vec} described a system of distributional word
embeddings called Skip-Grams with Negative Sampling (SGNS), which is more
popularly known by the name of its software implementation, {\em word2vec}.
(The {\em word2vec} software also implements another representation, Continuous
Bag-of-Words or CBOW, which is less often used for word similarity.)

In SGNS, a neural network with one hidden layer is trained to recognize words
that are likely to appear near each other. Its goal is to output a high value
when given examples of co-occurrences that appear in the data, and a low value
for negative examples where one word is replaced by a random word. The loss
function is weighted by the frequencies of the words involved and the distance
between them in the data. The word2vec software comes with SGNS embeddings
of text from Google News.

GloVe \cite{pennington2014glove} is an unsupervised learning algorithm that
learns a set of word embeddings such that the dot product of two words'
embeddings is approximately equal to the logarithm of their co-occurrence count.
The algorithm operates on a global word-word co-occurrence matrix, and
solves an optimization problem to learn a vector for each word, a separate
vector for each context (although the contexts are also words), and a bias
value for each word and each context. Only the word vectors are used for
computing similarity.

The embeddings that GloVe learns from data sources such as the Common
Crawl\footnote{\url{http://commoncrawl.org/}} are distributed on the GloVe web
page\footnote{\url{http://nlp.stanford.edu/projects/glove/}}. Here we evaluate
two downloadable sets of GloVe 1.2 embeddings, built from 42 billion and 840
billion tokens of the Common Crawl, respectively.

% NOTE: we may need to squish the next two paragraphs
There is some debate about whether GloVe or word2vec is better at representing
word meanings in general. GloVe is presented by \newcite{pennington2014glove}
as performing better than word2vec on word similarity tasks, but
\newcite{levy2015embeddings} finds that word2vec performs better with an
optimized setting of hyperparameters than GloVe does, when retrained with a
particular corpus.

In this paper, we focus only on the downloadable sets of term embeddings that
the GloVe and word2vec projects provide, not on re-running them with tuned
hyperparameters. Using this data makes it possible to reproduce their results
and compare directly to them, even when their preferred input data is not
publically available. We find that we can get very good results derived from
the downloadable embeddings, and that GloVe's downloadable embeddings outperform
word2vec's in this case, but a combination of them can perform even better.

\section{Methods}

\newcite{faruqui2015retrofitting} introduced the ``retrofitting'' procedure,
which adjusts dense matrices of embeddings (such as the GloVe output) to take
into account external knowledge from a sparse semantic network. They tried
various sources of external knowledge, and the one that was most helpful to
GloVe was PPDB.  We found using ConceptNet to be more effective, and that
further marginal improvements could be achieved on some evaluations by
combining ConceptNet and PPDB.

% FIXME: does this paragraph go somewhere else?
Our goal is to create a 300-dimensional vector space that represents terms based
on a combination of GloVe and word2vec's downloadable embeddings, and structured
data from ConceptNet and PPDB. The resulting vector space allows information to
be shared among these various representations, including words that were not in
the vocabulary of the original representations. This includes low-frequency words
and even words that are not in English.

\subsection{Standardizing Text}

As \newcite{levy2015embeddings} notes,
``[\ldots] much of the performance gains of word embeddings are due to certain
system design choices and hyperparameter optimizations, rather than the
embedding algorithms themselves.'' While it is presented as a negative result,
this simply emphasizes the importance of these system design choices.

Indeed, we have found that choices about how to handle terms and their
embeddings have a significant impact on evaluation results. One of these choices
involves how to pre-process words and phrases before looking them up, and
another involves the scale of the various features in the embeddings.


\subsubsection{Transforming and Aligning Vocabularies}

Different representations apply
different pre-processing steps, placing strings in different equivalence
classes. We can only properly combine these resources if these string
representations are comparable to each other.

% TODO: check this list

Pre-processing steps that various resources apply include: tokenizing text to
separate words from punctuation (which all inputs except GloVe 840B do),
joining multi-word phrases with underscores (ConceptNet and word2vec), removing
a small list of stopwords (ConceptNet only), folding the text to lowercase
(ConceptNet and GloVe 42B), replacing multiple digits with the character {\tt
\#} (word2vec only), lemmatizing English words to their root form using a
modification of WordNet's Morphy algorithm (ConceptNet only), and removing
accents from Spanish words (required for Spanish word similarity evaluations).

We adapt a text pre-processing function from ConceptNet to apply a combination
of all of these processes, yielding a set of standardized, language-tagged
labels. As an example, the text ``give an example'' becomes the standardized
form {\tt /c/en/give\_example}. Applying this combined pre-processing function
to all labels increases the alignment of the various resources while reducing
the size of the combined vocabulary.

Because the transformations are many-to-one, this has the effect that a single
transformed term can become associated with multiple embeddings in a single
vector space. We considered a few options for dealing with these merged terms:

\begin{enumerate}
\item Keep only the vector for the term with the highest frequency in the
    original data.
\item Average the vectors together.
\item Take a weighted average that favors higher-frequency terms.
\end{enumerate}

The weighted average turns out to be the best approach, as our evaluation will
show. The multiple rows contain valuable data that should not simply be
discarded, but lower-frequency rows tend to have lower-quality data.

When using pre-trained vectors, it is often the case that intermediate
computations that produced these vectors (such as word frequencies) are not
available. What we can do instead is to infer approximate word frequencies from
the fact that both GloVe and word2vec output their vocabularies in descending
order of frequency. We approximate the frequency distribution by assuming that
the tokens are distributed according to Zipf's law \cite{zipf1949human}: the
$n$th token in rank order has a frequency proportional to $1/n$. We use these
proportions in the weighted average when combining multiple embeddings.


\subsection{Normalization}

As briefly mentioned by \newcite{pennington2014glove}, $L_2$ normalization of
the columns (that is, the 300 features) of the GloVe matrix provides a notable
increase in performance. One effect of normalization is to increase the weight
of distinguishing features and reduce the impact of noisy features.  Features
are more distinguishing for the purpose of cosine similarity when they contain
a few large values and many small ones.

We find that $L_1$ normalization of GloVe performs even better than $L_2$
normalization. $L_1$ causes occasional large values to have a smaller impact on the norm
than $L_2$ normalization. When a learning method such as GloVe has provided
highly selective features, $L_1$ normalization allows us to use them more effectively
in measuring similarity.

\subsection{Retrofitting}

% TODO: restate less about how retrofitting works?

Retrofitting \cite{faruqui2015retrofitting} is a process of combining existing
word vectors with a semantic lexicon. While the original formulation expresses
the problem in terms of updates that propagate over a set of edges, we have
found it more convenient to express it and implement it in terms of an update
to a matrix.

The inputs to retrofitting are an initial dense matrix of term embeddings,
$W^0$, and a sparse matrix of known semantic relationships, $S$. Let the size
of the complete vocabulary be $m$ and the dimensionality of the embeddings be $n$.
$W^0$ is an $m \times n$ matrix containing the known term embeddings, with
rows of all zeroes for terms that are outside the vocabulary of the original embeddings.
$S$ is an $n \times n$ matrix containing positive weighted values for terms that
are known to be semantically related, 1 on the diagonal relating each term to
itself\footnote{
    These diagonal entries, or ``self-loops'' in the semantic network, are not strictly
    necessary, but we have found that they help the process converge faster.
    In Table~\ref{eval-variations}, we examine the effect of omitting the self-loops
    while running for the same number of steps.
}, and 0 otherwise.

Retrofitting on a matrix $W$ (whose rows are indicated by $w_i$) yields a final
matrix $W'$ (with rows indicated by $w'_i$) that minimizes

\begin{small}
$$
\Psi \left( W' \right) = \sum_{i=1}^v \left[
  \alpha_i \left\|  w'_i - w_i \right\| ^ 2
  + \sum_{j=1}^v S_{ij} \left\| w'_i - w'_j \right\| ^ 2
\right]
$$
\end{small}

where $\alpha$ is a vector that indicates the weight of a term in the original
vector space. For our purposes, we let $\alpha_i$ be 1 when word $i$ is in the
original embeddings' vocabulary, and 0 when it only appears in the semantic
network. (This neutralizes the effect of the all-zero rows in $W^0$.) We can
also represent these weights as a diagonal matrix $A$, where $A_{ii} =
\alpha_i$, allowing us to multiply matrices by it row-wise.

Similarly to the original formulation of retrofitting, $\Psi$ can be optimized
by a simple iterative updating method. We repeatedly update $W$ with:

$$
W^{k+1} = \mathrm{normalize}\left( \left( S W^k + W^0 A \right)\left( 1 + A \right)^{-1} \right)
$$

where $A$ is a matrix whose diagonal is the weight vector $\alpha$. The {\em normalize}
function applies $L_2$ normalization to each non-zero row of its argument, ensuring
that terms are always represented by unit vectors that can be used directly in
cosine similarity.

\subsection{ConceptNet as an Association Matrix}
% TODO: can this be trimmed?

In order to apply the expanded retrofitting method, we need to consider the data in
ConceptNet as a sparse, symmetric matrix of associations between terms. What
ConceptNet provides is more complex than that, as it connects terms with a
variety of not-necessarily-symmetric, labeled relations.

\newcite{havasi2010color} introduced a vector space embedding of ConceptNet,
``spectral association'', that disregarded the relation labels for the purpose
of measuring the relatedness of terms. Previous embeddings of ConceptNet, such
as that of \newcite{speer2008analogyspace}, preserved the relations but were
suited mostly for direct similarity and inference, not for relatedness. Because
most evaluation data for word similarity is also evaluating relatedness, unless
there has been a specific effort to separate them \cite{agirre2009similarity},
we erase the labels as in spectral association.

Negated relations and antonym relations, such as the relations expressed by
``a person does not have a tail'' and ``hot is the opposite of cold'', are
typically handled specially; a detail of spectral association is that negated
relations were assigned negative values in the matrix. Intuitively, negated
relations between two terms should not increase their relatedness. Perhaps
these relations should in fact decrease relatedness -- while ``hot'' and
``cold'' are similar in many ways, people providing judgments of relatedness
are likely to consider them less related because of the obvious contrast
between them.

However, association matrices are better behaved when they contain no negative
entries. Saying that vector $A$ should be a bit less related to vector
$B$ is very different from saying that it should be related to $-B$.
In our method, we simply omit all instances of negative and antonym
relations in ConceptNet.

Each remaining assertion in ConceptNet corresponds to two entries in a sparse
association matrix $S$. ConceptNet assigns a confidence score, or weight, to
each assertion. An assertion that relates term $i$ to term $j$ with weight $w$
will contribute $w$ to the values of $S_{ij}$ and $S_{ji}$. If another
assertion relates the same terms with a different relation, it will add to that
value. This constructs a symmetric matrix $S$, but the matrix we actually use
in retrofitting is the asymmetric $S'$, whose rows have been $L_1$-normalized.

Due to the structure of ConceptNet, there exists a large fringe of terms that are
poorly connected to other nodes. To make the sparse matrix and the size of the
overall vocabulary more manageable, we filter ConceptNet when building its
association matrix: we exclude all terms that appear fewer than 3 times, English
terms that appear fewer than 4 times, and terms with more than 3 words in them.

\subsection{Scaling Data Sources Within ConceptNet}

Each assertion in ConceptNet comes with a weight, indicating how strongly it
should be believed. A crowd-sourced dataset that collects the same fact from
many contributors, for example, should assign that fact a higher weight.

There is no absolute scale on these weights, and as a result, the weights can
be unbalanced when compared across different sources. An assertion entered once
into Open Mind Common Sense gets a weight of 1.0, for example, while an
assertion would have to be entered more than 50 times into the
game-with-a-purpose Verbosity to get the same weight.

In preliminary experimentation, we found that evaluation scores improved when
we increased the weights on Verbosity by a factor of 10. We followed up on that
by examining the average weights given by each data source, and found that the
average weight of a Verbosity assertion was 0.12, while other sources had
average weights between 1 and 2.

The correction we applied was to re-scale all the weights so that the average
weight within each data source was 1. This re-scaling had a positive effect
on all evaluations. For comparison, the ``Don't rebalance'' row of
Table~\ref{eval-variations} shows the effect of removing this rebalancing.

\section{Evaluation}

\begin{table*}[t]
\centering
\begin{tabular}{lllllrrrrrr}
\toprule
Label  &Embeddings   & Norm  & Text std. & Retrofit &       RW & MEN      &        WS &      SCWS &    RG-65 &    MC-30 \\
\midrule
       &GloVe 42B    & ---   & ---       & ---      &     .348 &     .740 &      .632 &      .440 &     .817 &     .777 \\
       &GloVe 42B    & ---   & CN5.4     & ---      &     .366 &     .760 &      .646 &      .444 &     .810 &     .762 \\
\bf G  &GloVe 42B    & $L_2$ & ---       & ---      &     .448 &     .816 &      .759 &      .595 &     .829 &     .836 \\
       &GloVe 42B    & $L_2$ & CN5.4     & ---      &     .490 &     .815 &      .765 &      .587 &     .779 &     .815 \\
       &GloVe 42B    & $L_1$ & ---       & ---      &     .457 &     .820 &      .766 &      .606 &     .826 &     .829 \\
       &GloVe 42B    & $L_1$ & CN5.4     & ---      &     .513 &     .834 &      .794 &      .619 &     .814 &     .828 \\
\midrule
       &GloVe 840B   & ---   & ---       & ---      &     .070 &     .728 &      .627 &      .441 &     .648 &     .696 \\
       &GloVe 840B   & ---   & CN5.4     & ---      &     .434 &     .803 &      .735 &      .552 &     .775 &     .787 \\
       &GloVe 840B   & $L_2$ & ---       & ---      &     .089 &     .768 &      .664 &      .496 &     .652 &     .666 \\
       &GloVe 840B   & $L_2$ & CN5.4     & ---      &     .493 &     .811 &      .760 &      .564 &     .717 &     .789 \\
       &GloVe 840B   & $L_1$ & ---       & ---      &     .091 &     .772 &      .667 &      .500 &     .653 &     .682 \\
\bf WR0&GloVe 840B   & $L_1$ & CN5.4     & ---      &     .512 &     .840 &      .798 &      .615 &     .774 &     .798 \\
\midrule
       &GloVe 840B   & ---   & CN5.4     & PPDB     &     .465 &     .814 &      .716 &      .598 &     .815 &     .815 \\
       &GloVe 840B   & ---   & CN5.4     & CN5.4    &     .506 &     .830 &      .734 &      .602 &     .842 &     .810 \\
       &GloVe 840B   & ---   & CN5.4     & Both     &     .507 &     .830 &      .731 &      .604 &     .838 &     .811 \\
       &GloVe 840B   & $L_2$ & CN5.4     & PPDB     &     .543 &     .829 &      .780 &      .633 &     .788 &     .819 \\
       &GloVe 840B   & $L_2$ & CN5.4     & CN5.4    &     .564 &     .841 &      .794 &      .631 &     .805 &     .836 \\
       &GloVe 840B   & $L_2$ & CN5.4     & Both     &     .566 &     .841 &      .794 &      .634 &     .801 &     .829 \\
       &GloVe 840B   & $L_1$ & CN5.4     & PPDB     &     .560 &     .852 &      .806 & {\bf .674}&     .821 &     .824 \\
\bf WR1&GloVe 840B   & $L_1$ & CN5.4     & CN5.4    &     .581 &{\bf .860}& {\bf .818}&      .668 &{\bf .852}&{\bf .845}\\
\bf WR2&GloVe 840B   & $L_1$ & CN5.4     & Both     &{\bf .584}&{\bf .860}&      .817 &      .671 &     .846 &     .842 \\
\bottomrule
\end{tabular}

\caption{
    Results on the word similarity task, shown as the Spearman rank correlation
    ($\rho$) between the learned embeddings and various human-annotated corpora.
    ``Norm'' indicates the norm applied to the columns of GloVe.
    ``Text std.'' indicates whether labels are left in their original form or
    standardized according to ConceptNet 5.4. ``Retrofit'' indicates which data
    is added using expanded retrofitting.
    Row {\bf G} reproduces the published GloVe results.
    {\bf WR1} and {\bf WR2} are the two best configurations of our system.
}
\label{eval-bigtable}
\end{table*}

\subsection{Word Similarity Datasets}

We evaluate our model's performance at identifying similar words using a
variety of word-similarity gold standards:

\begin{itemize}
\item MEN-3000 \cite{bruni2014men}, crowd-sourced similarity judgments for 3000
    word pairs.
\item The Stanford Rare Words (RW) dataset \cite{luong2013rw}, crowd-sourced
    similarity judgments for 2034 word pairs, with a bias toward uncommon words.
\item WordSim-353 \cite{finkelstein2001ws}, a widely-used corpus of similarity
    judgments for 353 word pairs.
\item SCWS*. SCWS \cite{huang2012scws} contains crowd-sourced similarity
    judgments for 2003 word pairs in the context of sentences, while SCWS* is
    the modification of it described by \newcite{luong2013rw}
    to disregard the context in order to evaluate
    systems that cannot make use of context. SCWS* discards the pairs of
    identical words that are only distinguished by context, leaving 1759 pairs.
\item RG-65 \cite{rubenstein1965rg}, a classic corpus of similarity judgments
    for 65 word pairs.
\item MC-30 \cite{miller1991mc}, similarity judgments for 30 word pairs.
\end{itemize}

To evaluate our ability to extend the vocabulary to other languages, we also
used translations of some of the above gold standards:

\begin{itemize}
\item RG-65 has been translated into German by \newcite{gurevych2005german},
    and into French by \newcite{joubarne2011french}.
\item WordSim-353 and MC-30 have been translated into Spanish, written in
    plain ASCII without accents, by \newcite{hassan2009crosslingual}. The
    same dataset also includes translations into Romanian and Arabic.
\end{itemize}

\subsection{Setting Aside Test Data}

In striving to maximize an evaluation metric, it is important to hold out some
data, to avoid overfitting to the data by modifying the algorithm and its
parameters. The metrics we focused on improving were our rank correlation with
MEN-3000, which emphasizes having high-quality representations of common words,
and RW, which emphasizes having a broad vocabulary.

MEN-3000 comes with a development/test split, where 1000 of the 3000 word pairs
are held out for testing. We applied a similar split to RW, setting aside a
sample of $1/3$ of its word pairs for testing.\footnote{
    We set aside every third row, starting from row 3, using the Unix command
    {\tt split -un r/3/3 rw.txt}. Similarly, we split on {\tt r/1/3} and
    {\tt r/2/3} and concatenated the results to get the remaining evaluation
    data.
}

Neither of these held-out test sets were used in our evaluations until the code
and parameters were finalized, shortly before submitting this article. While
Table~\ref{eval-bigtable} shows our correlation with only the test data on these
evaluations, Table~\ref{eval-dev-test} compares our performance on development
and test data.

\subsection{Results}

% TODO: compare to Rothe

\begin{table}[t]
\centering
\begin{tabular}{llrr}
\toprule
Label     & Method                 & RW [all] &      MEN \\
\midrule
          & Skip-gram (Levy)       &     .470 $(.435, .503)$ &     .774 $(.759, .787)$ \\
          & SVD (Levy)             &     .514 $(.481, .545)$ &     .777 $(.762, .791)$ \\
          & Retrofitting (Faruqui) &      ---                &     .796 $(.783, .809)$ \\
\bf G     & GloVe (Pennington)     &     .477 $(.443, .510)$ &     .816 $(.804, .828)$ \\
\bf WR0   & GloVe (our revision)   &     .528 $(.496, .559)$ &     .840 $(.829, .850)$ \\
\bf WR1   & Wide retrofitting      &     .601 $(.572, .628)$ &{\bf .860}$(.850, .869)$ \\
\bf WR2   & Wide retrofitting      &{\bf .589}$(.576, .632)$ &{\bf .860}$(.850, .869)$ \\
\bottomrule
\end{tabular}

\caption{
    Comparison between our vector-space word embeddings and previously-published
    results, on RW and MEN-3000, with 95\% confidence intervals. In order to compare
    with previous results, we use all of the RW data, not just the 1/3 of it we set
    aside for testing.
}
\label{compare-others}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{lrrrr}
\toprule
Label   & WS [es] & MC [es] & RG [de] & RG [fr] \\
\midrule
\bf R   &         --- &       .591 &       .603 &       .606 \\
\bf WR1 &    \bf .444 &   \bf .765 &       .673 &       .781 \\
\bf WR2 &        .441 &       .762 &   \bf .677 &   \bf .788 \\
\bottomrule
\end{tabular}

\caption{
    Multilingual evaluation results for Spanish (es), German (de), and French
    (fr), on translations of WS-353, MC-30, and RG-65. Row {\bf R} contains the
    published results of retrofitting
    Universal WordNet onto skip-gram embeddings learned from Wikipedia.
    {\bf WR1} and {\bf WR2} are our system.
}
\label{eval-multilingual}
\end{table}

\begin{figure}
\begin{tikzpicture}[scale=.9]
    \begin{axis}[enlargelimits=0.2, xlabel={$\rho$ correlation with MEN-3000 (common words)}, ylabel={$\rho$ correlation with RW (rare words)}]
        \addplot[
          scatter,mark=*,only marks,nodes near coords,
          point meta=explicit symbolic,
          visualization depends on={value \thisrow{anchor}\as\myanchor},
          every node near coord/.append style={anchor=\myanchor}
        ]
        table[meta=label] {
            x      y     label             anchor
            .777   .514  {Levy SVD}        west
            .774   .470  {Levy skip-gram}  west
            .8163  .4767 {GloVe}           west
            .8400  .5280 {WR0}             west
            .8599  .5850 {WR1}             west
            .8597  .5887 {WR2}             south
        };
    \end{axis}
\end{tikzpicture}
\caption{
    Systems discussed in this paper, plotted according to their $\rho$
    correlation with MEN-3000 and RW.
}
\label{compare-graph}
\end{figure}


We have labeled some of the rows of Table~\ref{eval-bigtable} as particular systems
that we would like to compare. System {\bf G} represents a configuration of
GloVe that was evaluated by \newcite{pennington2014glove}. We ran the evaluation
using their provided data, and successfully reproduced their results.

{\bf WR1} and {\bf WR2} are two preferred configurations of our system. {\bf WR1}
applies all the techniques described in this paper as it retrofits with
ConceptNet; {\bf WR2} is the same, but retrofits with a combination of PPDB and
ConceptNet. Both systems perform very well across all word-similarity
evaluations, and it is inconclusive which one is better. We will focus on
system {\bf WR1} for further comparison, because it is simpler.

The numerals in the names {\bf WR1} and {\bf WR2} refer to the number of
additional data sources that were added to the original data using our expanded
version of retrofitting. Another interesting result to compare is {\bf WR0},
the best system that we created without retrofitting anything. This system
simply involves transforming the rows and columns of the existing GloVe 840B
matrix, showing that some of the improvements from this paper can be realized
without introducing any additional data.

It is difficult to compare our system to \newcite{rothe2015} because they only
evaluated on SCWS. Their system can use the full SCWS, because it has a
representation of word contexts, and as such it achieves a higher score than
our system gets on SCWS* without contexts.

Table~\ref{eval-multilingual} shows the performance of these systems on
gold standards that have been translated to other languages, in comparison to
the multilingual results published by \newcite{faruqui2015retrofitting}.
These systems perform well in German, French, and Spanish, even though only
German has a data source designed for it in ConceptNet. French and Spanish terms
are only available because of translations in the English Wiktionary, and
indirect translations via Japanese in JMDict.

In contrast, we also briefly tried evaluating on the Arabic and Romanian
translations from \newcite{hassan2009crosslingual}. These languages are not
currently well-represented in ConceptNet, and their preliminary evaluation
results were quite poor.

\subsection{Comparisons to Other Published Results}

\begin{table*}[t]
\centering
\begin{tabular}{lrrrrrr}
\toprule
Label   & RW [dev] & RW [test] & RW [all] & MEN [dev] & MEN [test] & MEN [all]\\
\midrule
\bf G   &    .489  &    .448  &    .477  &    .813  &    .816 &    .814 \\
\bf WR0 &    .536  &    .512  &    .528  &    .841  &    .840 &    .841 \\
\bf WR1 &    .587  &    .581  &    .585  &\bf .858  &\bf .860 &\bf .859 \\
\bf WR2 &\bf .591  &\bf .584  &\bf .589  &    .857  &\bf .860 &    .858 \\
\bottomrule
\end{tabular}

\caption{
    A comparison of evaluation results between the ``dev'' datasets that were
    used in development, and the held-out ``test'' datasets, for the systems
    labeled in Table~\ref{eval-bigtable}.
}
\label{eval-dev-test}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{lrrrrrr}
\toprule
Modification & RW [dev] & MEN [dev] & WS-353 & SCWS &  RG-65 &  MC-30 \\
\midrule
{\bf Unmodified}                     & \bf .587 & \bf .858 & \bf .818 &  .668 &   .852 &   .845 \\
Use first row instead of row-merging & .563 &      .827 &    .787 &  .649 &   .822 &   .794 \\
Unweighted row-merging               & .526 &      .844 &    .751 &  .586 &   .841 &   .836 \\
No self-loops in retrofitting        & .570 &      .855 &    .790 & \bf .676 & \bf .873 & \bf .853 \\
Don't rebalance ConceptNet sources   & .584 &      .854 &    .816 &  .668 &   .846 &   .842 \\
\bottomrule
\end{tabular}
\caption{
    The effects of various modifications to the embeddings of system {\bf WR1}.
    RW and MEN-3000 were evaluated using their development sets here,
    not the held-out test data.
}
\label{eval-variations}
\end{table*}

In Table~\ref{compare-others}, we compare our results on the RW and MEN-3000
datasets to the best published results that we know of.
\newcite{levy2015embeddings} present results including an SVD-based method
that scores $\rho = .514$ on the RW evaluation, as well as an implementation
of skip-grams with negative sampling (SGNS), originally introduced by
\newcite{mikolov2013word2vec}. We also compare to the original results from
GloVe's 42-billion-token dataset, and the best MEN-3000 result from
\newcite{faruqui2015retrofitting}.

In relative terms, the performance of our system {\bf WR2} outperforms the best
published RW score (Levy's SVD) by 14.6\%, or 13.6\% if we use our score on only
the test set. Both {\bf WR1} and {\bf WR2} outperform GloVe's result on
MEN-3000 by 5.4\%.

\section{Discussion}

\subsection{Standardizing Term Texts}
% TODO: rewrite to "the importance of text standardization" or something

When we applied ConceptNet's lemmatizer to the labels of GloVe's 840B-token
dataset, we found an unexpected benefit of lemmatization: it caused a very
large performance increase on GloVe 840B even before combining it with any
other data.

When we run our evaluation without lemmatization on the GloVe 42B-token
embeddings, we reproduce the results published in \cite{pennington2014glove}.
This same evaluation, when run on the 840B-token embeddings, produces
surprisingly poor results, implying that some of these rare words appear in
forms that have low-quality embeddings. When we lemmatize the GloVe labels and
combine them using the Zipf estimate, however, the result outperforms the
42B-token embeddings.

It's important to note that we are not changing the evaluation data by using
a lemmatizer; we are only changing the way we look up its embeddings in the
vector space that we are evaluating. For example, if an evaluation requires
similarities for the words ``dry'' and ``dried'' to be ranked differently, or
the words ``polish'' and ``Polish'', the lemmatized system will rank them the
same, and will be penalized in its Spearman correlation.
However, the benefits of lemmatization when evaluating semantic similarity
appear to far outweigh the drawbacks, especially on very large and somewhat
messy data sets such as GloVe 840B.

\subsection{Varying the System}

Some of the procedures we implemented in creating systems {\bf WR1} and
{\bf WR2} require some justification. To show the benefits of certain decisions,
such as adding self-loops or the way we choose to merge rows of GloVe, we have
evaluated what happens to the system in the absence of each decision. These
evaluations appear in Table~\ref{eval-variations}. These evaluations were
part of how we decided on the best configuration of the system, so they were
run on the development sets of RW and MEN-3000, not the held-out test sets.

In Table~\ref{eval-variations}, we can see that the choice of how to merge rows
of GloVe that get the same standardized label makes a large difference.  Recall
that the method we ultimately used was to assign each row a pseudo-frequency
based on Zipf's law, and then take a weighted average based on those
frequencies.  The results drop noticeably when we try the other proposed
methods, which are taking only the first (most frequent) row that appears, or
taking the unweighted average of the rows.

In the retrofitting procedure, we made the decision to add ``self-loops'' that
connect each term to itself, hypothesizing that this would help stabilize the
representations of terms that are outside the original vocabulary of GloVe.
Reversing this decision causes a noticeable drop in performance on RW, the
evaluation that is most likely to involve words that were poorly represented or
unrepresented in GloVe. On the other hand, the lack of self-loops seems to {\em
increase} its score on SCWS, and on the smaller evaluations RG-65 and MC-30.

Rebalancing the weights of the data sources within ConceptNet seems to have
been a mild improvement. The ``Don't rebalance'' row of Table~\ref{eval-variations}
confirms what we saw when we introduced rebalancing,
as leaving the weights unbalanced decreases the scores slightly.

In separate experimentation, we found that when we separate ConceptNet into its
component datasets and drop each one in turn, the effects on the evaluation
results are mostly quite small. There is no single dataset that acts as the
``keystone'' without which the system falls apart, but one dataset ---
Wiktionary --- unsurprisingly has a larger effect than the others, because it
is responsible for most of the assertions. Of the 5,631,250 filtered assertions
that come from ConceptNet, 4,244,410 of them are credited to Wiktionary.

Without Wiktionary, the score on RW drops from .587 to .541. However, at the
same time, the MEN-3000 score {\em increases} from .858 to .865. The system
continues to do what it is designed to do without Wiktionary, but there seems to
be a tradeoff in performance on rare words and common words involved.

\subsection{The Limits of Similarity Judgments}

\newcite{bruni2014men} describe a substitute for inter-annotator agreement on
their MEN-3000 dataset:

\begin{quote}
``The Spearman correlation of the two authors is at 0.68, the correlation of their
average ratings with the MEN scores is at 0.84. On the one hand, this high
correlation suggests that MEN contains meaningful semantic ratings. On the
other, it can also be taken as an upper bound on what computational models can
realistically achieve when simulating the human MEN judgments.''
\end{quote}

Our model achieves a MEN score of \scoreMEN{} on the
held-out test data, putting its performance above that upper bound. In many
circumstances, this would be an indication that the model has overfit to the
MEN data. However, we have tried to eliminate the possibility of overfitting.
The MEN data is never used as an input to the system; the same system also
achieves good results on all other data sets; and we did not use or even
look at the held-out data when setting parameters. Strikingly, the evaluation
score on MEN goes {\em up} when the held-out data is evaluated instead.

We postulate instead that similarity evaluations become easier to match
computationally when averaged over the judgments of many people. Notice, for
example, that the two MEN-3000 authors who provided their own ratings
agreed more with the average of the crowd than they did with each other.
If more people's judgments had been averaged in with the authors', perhaps the
correlation would have been even higher. We suspect that, while these results
may be approaching it, the true upper bound of the MEN evaluation is higher than
claimed.

\subsection{Future Work}

% TODO: make these paragraphs flow together?

One aspect of our method that clearly has room for improvement is the fact that
we disregard the labels on relations in ConceptNet, and exclude antonym
relations altogether. There is valuable knowledge there that we might be able
to take into account with a more sophisticated extension of retrofitting, one
that goes beyond simply knowing that particular words should be related, to
handling them differently based on {\em how} they are related.

It was convenient, but not entirely correct, to assume that all of GloVe's data
from the Common Crawl is in English. The Common Crawl is in a wide variety of
languages, just as the Web itself is. In our system, non-English terms are
represented only by links in ConceptNet, but we could have a better starting
point if multilingual terms were represented in the GloVe vectors in the first
place. It could be useful to re-run GloVe in a way that distinguishes languages,
using the metadata on Web pages when available and language-detection heuristics
otherwise.

We believe that the variety of data sources represented in ConceptNet helped to
improve evaluation scores by expanding the domain of the system's knowledge.
There's no reason the improvement needs to stop here. It is quite likely that
there are more sources of ``linked data'' that could be included, or further
standardizations that could be applied to the text to align more data. In
short, these results can probably be surpassed soon. As the second author
observed while running evaluations, ``It seems like every time we do {\em
anything} to the data, the results get better.''

\section{Reproducing These Results}

We aim for these results to be reproducible and reusable by others. The code
that we ran to produce the results in this paper is available in a GitHub
repository at [{\em currently private pending blind review}]. We have tagged
this revision as [{\em TBD}], as we intend to continue to revise and improve the
code afterward. The data files are available at [{\em TBD}], and the README file
in the repository explains how to use {\tt git-annex} to connect the code
with a version-controlled snapshot of the data.

\section*{Acknowledgements}

\begin{CJK*}{UTF8}{min}
\bibliography{wordsim_paper}
\end{CJK*}

\end{document}
