\def\year{2015}
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{url}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title Improving Word Vectors by Adding Lexical Knowledge
/Author Rob Speer, Joshua Chin, and Catherine Havasi}
\setcounter{secnumdepth}{0}
\bibliographystyle{aaai}

\title{Improving Word Vectors by Adding Lexical Knowledge}
\author{Robert Speer\\
    Luminoso Technologies, Inc.\\
    675 Massachusetts Ave.\\
    Cambridge, MA 02139\\
    \texttt{rspeer@luminoso.com}
\And
    Joshua Chin\\
    {\em academic address goes here}
\And
    Catherine Havasi\\
    Luminoso Technologies, Inc.\\
    675 Massachusetts Ave.\\
    Cambridge, MA 02139\\
    \texttt{havasi@luminoso.com}
}

\begin{document}
\maketitle
\begin{abstract}
We did stuff to GloVe and ConceptNet and got better stuff.
\end{abstract}

\section{Introduction}
blah blah blah

\section{Background}

\subsection{GloVe}
\ldots introduce GloVe \ldots % TODO

Learning processes such as GloVe produce results that can conveniently be
re-used in other projects, because they output a labeled matrix of the learned
embeddings of each word in the vocabulary.
\citeauthor{faruqui2014retrofitting} \shortcite{faruqui2014retrofitting}
introduced the ``retrofitting'' procedure, which adjusts such dense matrices of
embeddings based on external knowledge from a sparse semantic network.

\citeauthor{faruqui2014retrofitting} used PPDB \cite{ganitkevitch2013ppdb} as
the external source of semantic knowledge, but we found that ConceptNet
\cite{speer2012conceptnet}, a resource which combines a variety of forms of
crowd-sourced and expert-created knowledge, should fit well with this process
while greatly increasing the scope of its vocabulary.

\subsection{ConceptNet}
ConceptNet 5 \cite{speer2012conceptnet} is a semantic network of terms
connected by labeled relations. Its terms are words or multiple-word phrases
in a variety of natural language. For continuity with previous work,
these terms are often referred to as {\em concepts}.

ConceptNet originated as a machine-parsed version of the early crowd-sourcing
project called Open Mind Common Sense (OMCS) \cite{singh2002omcs}, and has expanded
to include several other data sources, both crowd-sourced and expert created.
In the latest version, ConceptNet 5.4, the data sources it includes are:

\begin{itemize}
\item Knowledge collected as English sentences on the original OMCS website,
    and later parsed with a pattern-matching parser
\item Sister projects to OMCS that collected similar sentences in Portuguese,
    Dutch, and some Korean and Japanese \cite{anacleto2006portuguese}
    \cite{eckhardt2008dutch} \cite{TODO-globalmind}
\item ``Games with a purpose'' that collect relational knowledge, such as
    Verbosity \cite{vonahn2006verbosity} in English, {\em nadya.jp}
    \cite{TODOnadya} in Japanese, and the PTT pet game \cite{kuo2009petgame}
    in Taiwanese Chinese
\item WordNet 3.0 \cite{fellbaum1998wordnet}
\item A parsed version of Wiktionary, whose parser is developed as part of the
    ConceptNet 5 codebase
\item Verbosity \cite{suranaTODOverbosity}, an online ``game with a purpose''
    that collected English relational knowledge
\item JMDict \cite{TODOjmdict}, a Japanese multilingual dictionary
\item OpenCyc \cite{TODOcyc} as interpreted via Umbel \cite{TODOumbel}
\end{itemize}

ConceptNet 5.4 also includes a mapping of DBPedia \cite{auer2007dbpedia} into
its term space, but this is kept separate in such a way that it is not used
in this paper.

% TODO ConceptNet image


\section{Methods}

\subsection{Standardizing text}

% TODO: condense this as much as possible

The way we choose to represent words and compare them for equality may seem
like a trivial matter, but it can significantly affect evaluation results.
Additionally, we can only properly combine two resources with a method such
as retrofitting if their string representations are comparable to each other.

The operations we apply to text would usually be called ``normalization'',
but that word also refers to what we'll be doing to our vectors, so both
here and in the code we call it ``standardization'' instead.

\subsubsection{How GloVe is standardized}

The 42-billion-token version of GloVe, whose results are published in
\cite{pennington2014glove}, has applied some simple standardizations
to the text it extracts from Common Crawl:

\begin{itemize}
\item The text is tokenized with the Stanford parser. Only single tokens are
    used in GloVe.
\item The words are lowercased using a Unicode case-conversion
    algorithm.
\item Although not all of Common Crawl is in valid UTF-8 or even in a known
    encoding, only words in valid UTF-8 are written to the file.
\end{itemize}

It is important to keep these standardizations in mind and also apply them to
the word-similarity datasets, such as lowercasing the word ``OPEC'' in {\sc
wordsim-353}.

The 840B-token dataset is not lowercased, as described in
\cite{pennington2014glove}, but it is also not all in UTF-8. It is possible
that the tokens were written to the file exactly as they were found in
Common Crawl, as bytes of ``encoding salad'', as one might expect from
scraping arbitrary Web pages. They are still tokenized in a way that is
reasonable for English.

\subsubsection{How ConceptNet is standardized}

An operation that is common in Web design is to ``slugify'' a title or
headline, converting it to a reduced, not-necessarily-grammatical form
that fits into a URL while keeping all the key words. ConceptNet's
normalization is similar: it converts a term into a form that fits into
a URI.

The standardizations may seem byzantine, but their purpose is to align many
different resources -- particularly on English terms, where they most often
align. Here are the standardizations applied to English terms in ConceptNet
5.4:

\begin{itemize}
\item The words and phrases that the data sources provide are first run
    through the {\tt ftfy.fix\_text} function, in version 4.0 of the Python
    module {\tt ftfy}.\footnote{
        \url{http://github.com/LuminosoInsight/python-ftfy}
    } This corrects some Unicode encoding glitches, as well as applying
    NFC normalization.
\item The text is tokenized using Treebank tokenization
    \cite{marcus1993treebank}.
\item Each token is lemmatized using a modification of WordNet's Morphy
    algorithm, which appears in the {\tt conceptnet5.language.english}
    module of ConceptNet 5.4's Python code. Because most data sources are not
    tagged, it uses heuristics to choose which part of speech to normalize
    the word as, plus exceptions that allow it to choose more common roots
    when Morphy would choose a rare one.
\item Tokens whose first character is not alphanumeric (according to Python
    3's {\tt isalnum}) are removed.
\item A very small list of stopwords (``a'', ``an'', and ``the'') are
    removed, as well as ``to'' if it is the first word.
\item Terms that would become the empty string due to the above steps are
    returned to their original text.
\item Spaces and slashes are replaced by underscores.
\item The text is lowercased according to Python 3, which uses Unicode's
    context-sensitive algorithm.
\item The resulting standardized text is tagged with its language and turned
    into a URI: the phrase ``giving an example'', for example, becomes
    {\tt /c/en/give\_example}.
\end{itemize}

The complete implementation can be found in the
{\tt conceptnet5.nodes.normalized\_concept\_uri} function of ConceptNet 5.4.

The URIs are not entirely meant for human consumption, and are
sometimes not grammatically accurate -- the term ``ground zero'' becomes
{\tt /c/en/grind\_zero}, for example -- but the purpose is to unify
surface texts that may appear in slightly different forms in different
resources.

When aligning GloVe and ConceptNet, then, we apply these standardizations on
top of what has already been applied to GloVe, turning its tokens into
ConceptNet URIs. The tokens are assumed to be in English.

A possible drawback to this standardization process is that we will not be able
to evaluate the result on Mikolov et al.'s syntactic analogy task
\cite{mikolov2013word2vec}, as many of the analogies would become trivial
analogies of the form ``A is to A as B is to B''. (We will discuss below
why analogies are not a goal of this system anyway.)

\section{Evaluation}



\section{Discussion}

\subsection{Analogies are not our goal}
\label{analogies-meh}

The analogy evaluation of \cite{mikolov2013word2vec} is an interesting
emergent property to demonstrate of a system that has no syntactic or
relational knowledge built into it. However, at the point where the system
{\em does} have more knowledge built into it, it becomes somewhat of
a meaningless exercise.

The syntactic analogies \ldots

\section{References}
\bibliography{wordsim_paper}

\end{document}
