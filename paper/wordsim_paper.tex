\def\year{2015}
\def\thetitle{Improving Word Vectors by Adding Lexical Knowledge}

\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{url}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title \thetitle
/Author Rob Speer, Joshua Chin, and Catherine Havasi}
\setcounter{secnumdepth}{0}
\bibliographystyle{aaai}

\title{\thetitle}
\author{Robert Speer\\
    Luminoso Technologies, Inc.\\
    675 Massachusetts Ave.\\
    Cambridge, MA 02139\\
    \texttt{rspeer@luminoso.com}
\And
    Joshua Chin\\
    {\em academic address goes here}
\And
    Catherine Havasi\\
    Luminoso Technologies, Inc.\\
    675 Massachusetts Ave.\\
    Cambridge, MA 02139\\
    \texttt{havasi@luminoso.com}
}

\begin{document}
\maketitle
\begin{abstract}
We did stuff to GloVe and ConceptNet and got better stuff.
\end{abstract}

\section{Introduction}
blah blah blah

\section{Background}

\subsection{GloVe}
\ldots introduce GloVe \ldots % TODO

Learning processes such as GloVe produce results that can conveniently be
re-used in other projects, because they output a labeled matrix of the learned
embeddings of each word in the vocabulary.
\citeauthor{faruqui2014retrofitting} \shortcite{faruqui2014retrofitting}
introduced the ``retrofitting'' procedure, which adjusts such dense matrices of
embeddings based on external knowledge from a sparse semantic network.

\citeauthor{faruqui2014retrofitting} used PPDB \cite{ganitkevitch2013ppdb} as
the external source of semantic knowledge, but we found that ConceptNet
\cite{speer2012conceptnet}, a resource which combines a variety of forms of
crowd-sourced and expert-created knowledge, should fit well with this process
while greatly increasing the scope of its vocabulary.

\subsection{ConceptNet}
ConceptNet 5 \cite{speer2012conceptnet} is a semantic network of terms
connected by labeled relations. Its terms are words or multiple-word phrases
in a variety of natural language. For continuity with previous work,
these terms are often referred to as {\em concepts}.

ConceptNet originated as a machine-parsed version of the early crowd-sourcing
project called Open Mind Common Sense (OMCS) \cite{singh2002omcs}, and has expanded
to include several other data sources, both crowd-sourced and expert created.
In the latest version, ConceptNet 5.4, the data sources it includes are:

\begin{itemize}
\item Knowledge collected as English sentences on the original OMCS website,
    and later parsed with a pattern-matching parser
\item Sister projects to OMCS that collected similar sentences in Portuguese,
    Dutch, and some Korean and Japanese \cite{anacleto2006portuguese}
    \cite{eckhardt2008dutch} \cite{TODO-globalmind}
\item ``Games with a purpose'' that collect relational knowledge, such as
    Verbosity \cite{vonahn2006verbosity} in English, {\em nadya.jp}
    \cite{TODOnadya} in Japanese, and the PTT pet game \cite{kuo2009petgame}
    in Taiwanese Chinese
\item WordNet 3.0 \cite{fellbaum1998wordnet}
\item A parsed version of Wiktionary, whose parser is developed as part of the
    ConceptNet 5 codebase
\item Verbosity \cite{suranaTODOverbosity}, an online ``game with a purpose''
    that collected English relational knowledge
\item JMDict \cite{TODOjmdict}, a Japanese multilingual dictionary
\item OpenCyc \cite{TODOcyc} as interpreted via Umbel \cite{TODOumbel}
\end{itemize}

ConceptNet 5.4 also includes a mapping of DBPedia \cite{auer2007dbpedia} into
its term space, but it is not used for word associations, so its presence does
not affect the results in this paper.

% TODO: ConceptNet image


\section{Methods}

Here, we create a matrix that assigns embeddings in a 300-dimensional vector
space to terms by combining data from ConceptNet and GloVe. The resulting word
embeddings incorporate the strengths of both ConceptNet and GloVe, and extend
their vocabulary to the union of ConceptNet and GloVe, allowing embeddings that
were originally only trained on English to be extended to other languages.

\subsection{Standardizing text}

The way we choose to represent words and compare them for equality may seem
like a trivial matter, but it can significantly affect evaluation results.
Additionally, we can only properly combine two resources with a method such
as retrofitting if their string representations are comparable to each other.

The operations we apply to text would usually be called ``normalization'',
but that word also refers to what we'll be doing to our vectors, so both
here and in the code we call it ``standardization'' instead.

\subsubsection{How ConceptNet is standardized}

Here are the standardizations applied to all terms in ConceptNet 5.4:

\begin{itemize}
\item The words and phrases that the data sources provide are first run
    through the {\tt ftfy.fix\_text} function, in version 4.0 of the Python
    module {\tt ftfy}.\footnote{
        \url{http://github.com/LuminosoInsight/python-ftfy}
    } This corrects some Unicode encoding glitches, as well as applying
    NFC normalization.
\item The text is tokenized using a regular expression that splits it at
    characters whose major Unicode category is Z (whitespace), P (punctuation),
    S (symbols), or C (control characters).
\item If the text is English, each token is lemmatized using a modification of
    WordNet's Morphy algorithm, which appears in the {\tt conceptnet5.language.english}
    module of ConceptNet 5.4's Python code. This converts inflected words such
    as ``creating'' to their root words that appear in WordNet, such as
    ``create''.
    % Because most data sources are not tagged with parts of speech,
    % it uses heuristics to choose which part of speech to lemmatize the word as,
    % plus exceptions that allow it to choose more common roots when Morphy would
    % give multiple options.
\item If the text is English, a very small list of stopwords (``a'', ``an'',
    and ``the'') are removed, as well as ``to'' if it is the first word.
\item Spaces and slashes are replaced by underscores.
\item The text is lowercased according to the Unicode case-mapping algorithm
    that is implemented in Python 3.4.
\item The resulting standardized text is tagged with its language and turned
    into a URI: the phrase ``giving an example'', for example, becomes
    {\tt /c/en/give\_example}.
\end{itemize}

The complete implementation can be found in the
{\tt conceptnet5.nodes.normalized\_concept\_uri} function of ConceptNet 5.4.

The URIs are not entirely meant for human consumption, and are
sometimes not grammatically accurate -- the term ``ground zero'' becomes
{\tt /c/en/grind\_zero}, for example -- but the purpose is to unify
surface texts that may appear in slightly different forms in different
resources.

A drawback to this standardization process is that we will not be able
to evaluate the result on Mikolov et al.'s syntactic analogy task
\cite{mikolov2013word2vec}, as many of the analogies would become trivial
analogies of the form ``A is to A as B is to B''.

\subsubsection{How GloVe is standardized}

The 42-billion-token version of GloVe, whose results are published in
\cite{pennington2014glove}, has applied some simple standardizations
to the text it extracts from Common Crawl:

\begin{itemize}
\item The text is tokenized with the Stanford parser. Only single tokens are
    used in GloVe.
\item The words are lowercased using a Unicode case-conversion
    algorithm.
\item Although not all of Common Crawl is in valid UTF-8 or even in a known
    encoding, only words in valid UTF-8 are written to the file.
\end{itemize}

When evaluating GloVe, it is important to keep these standardizations in mind
and also apply them to the word-similarity datasets, such as lowercasing the
word ``OPEC'' in {\sc wordsim-353}.

The 840B-token dataset is not lowercased, as described in
\cite{pennington2014glove}, but it is also not all in UTF-8. It is possible
that the tokens were written to the file exactly as they were found in
Common Crawl, as bytes of ``encoding salad'', as one might expect from
scraping arbitrary Web pages. They are still tokenized in a way that is
reasonable for English.

\subsubsection{Aligning ConceptNet and Glove}

ConceptNet's standardization process is stronger than GloVe's, so we align them
by applying the ConceptNet standardizations to the GloVe labels as well, turning
its tokens into ConceptNet URIs. The GloVe tokens are assumed to be in English.

This has the effect that many separate rows of the GloVe matrix get the same
label, such as when the same word appears with a different capitalization. A
few options for dealing with these merged labels come to mind:

\begin{enumerate}
\item Keep only the row with the highest frequency in GloVe.
\item Average the rows together.
\item Take a weighted average that favors higher-frequency rows.
\end{enumerate}

The weighted average turns out to be the best approach, as our evaluation will
show. The multiple rows contain valuable data that should not simply be
discarded, but lower-frequency rows tend to have lower-quality data.

The word frequency data is not distributed with the GloVe matrix, but
the rows are clearly in descending order of frequency. We approximate the
frequency distribution by assuming that the tokens are distributed according to
Zipf's law \cite{TODOzipf}: the $n$th token in rank order has a frequency
proportional to $1/n$.

\section{Evaluation}

% TODO: lots of stuff
% TODO: evaluate other row-combining metrics
% TODO: retrofit PPDB instead of ConceptNet and compare

\section{Discussion}

\subsection{Analogies are not our goal}
\label{analogies-meh}

The analogy evaluation of \cite{mikolov2013word2vec} is an interesting
emergent property to demonstrate of a system that has no syntactic or
relational knowledge built into it. However, at the point where the system
{\em does} have more knowledge built into it, it becomes somewhat of
a meaningless exercise.

The syntactic analogies \ldots

\bibliography{wordsim_paper}

\end{document}
